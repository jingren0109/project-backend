{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "The training process involved three different models trained on different subsets of the loan data. The first model was trained on the original loan data, excluding the new metric columns and joint application data. The second and third models were trained on the same data as the first model, but included the new metric columns and joint application data as one-hot encoded features.\n",
    "\n",
    "All three models used a similar neural network architecture with three hidden layers of varying sizes and 30% dropout regularization after each layer. The models were trained using the mean squared logarithmic error loss function and the Adam optimizer.\n",
    "\n",
    "The first model was trained for 10 epochs with a batch size of 128, while the second and third models were trained for 10 epochs with batch sizes of 64 and 32, respectively. The third model was the final model used for prediction and was trained on the entire dataset without validation.\n",
    "\n",
    "The trained model was saved in a file named \"loan_risk_model\", and the data transformer used to preprocess the data was saved in a file named \"data_transformer.joblib\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The DataFrame contains 2,260,701 observations and each observation has 151 variables."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_row = pd.read_csv(\n",
    "  \"./model/accepted_2007_to_2018Q4.csv\",\n",
    "  low_memory=False,\n",
    ")\n",
    "\n",
    "data_row.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_row.head(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Total percentage of null values in the data\n",
    "# (data_row.isnull().sum().sum())/(data_row.shape[0]*data_row.shape[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This code reads in the LCDataDictionary.xlsx file, which is a data dictionary that provides descriptions for each of the variables in the LendingClub loan dataset.\n",
    "This is a helpful way to understand what each variable represents in the dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "variable_definitions = pd.read_excel(\"https://resources.lendingclub.com/LCDataDictionary.xlsx\")\n",
    "\n",
    "# Drop blank rows, strip white space, set the \"LoanStatNew\" column as the index, and converting the \"Description\" column to a Python dictionary.\n",
    "variable_definitions.dropna(axis=\"index\", inplace=True)\n",
    "variable_definitions = variable_definitions.applymap(lambda x: x.strip())\n",
    "variable_definitions.set_index(\"LoanStatNew\", inplace=True)\n",
    "dictionary = variable_definitions[\"Description\"].to_dict()\n",
    "# Rename \"verified_status_joint\" to \"verification_status_joint\".\n",
    "dictionary[\"verification_status_joint\"] = dictionary.pop(\"verified_status_joint\")\n",
    "\n",
    "# Print out each column name in the data_row and its corresponding description from the data dictionary\n",
    "for col in data_row.columns:\n",
    "  print(f\"•{col}: {dictionary[col]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here are the descriptions of each of the columns:\n",
    "\n",
    "term: The number of payments on the loan. Values are in months and can be either 36 or 60.\n",
    "installment: The monthly payment owed by the borrower if the loan is funded.\n",
    "total_rec_prncp: The total principal amount of the loan that has been paid off by the borrower.\n",
    "total_rec_int: The total interest amount that has been paid by the borrower.\n",
    "total_rec_late_fee: The total late fee amount that has been paid by the borrower.\n",
    "recoveries: The post charge off gross recovery amount.\n",
    "collection_recovery_fee: The post charge off collection fee.\n",
    "\n",
    "Here is the formula we can use to calculate:\n",
    "\n",
    "Expected Return = term x installment\n",
    "Amount Received = total_rec_prncp + total_rec_int + total_rec_late_fee + recoveries - collection_recovery_fee\n",
    "Fraction of Expected Return Recovered = Amount Received / Expected Return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_columns = [\"term\", \"installment\", \"total_rec_prncp\", \"total_rec_int\", \"total_rec_late_fee\", \"recoveries\", \"collection_recovery_fee\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some columns contain irrelevant demographic data or data that was not created until after a loan was accepted, and thus those columns need to be removed. The column \"emp_title\" (the applicant's job title) is potentially relevant in the context of a loan, but it has too many unique values to be useful. We can drop this column for now."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_row[\"emp_title\"].nunique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dropped_columns = [\"id\", \"member_id\", \"funded_amnt\", \"funded_amnt_inv\", \"int_rate\", \"grade\", \"sub_grade\", \"emp_title\", \"pymnt_plan\", \"url\", \"desc\", \"title\", \"zip_code\", \"addr_state\", \"initial_list_status\", \"out_prncp\", \"out_prncp_inv\", \"total_pymnt\", \"total_pymnt_inv\", \"last_pymnt_d\", \"last_pymnt_amnt\", \"next_pymnt_d\", \"last_credit_pull_d\", \"last_fico_range_high\", \"last_fico_range_low\", \"policy_code\", \"hardship_flag\", \"hardship_type\", \"hardship_reason\", \"hardship_status\", \"deferral_term\", \"hardship_amount\", \"hardship_start_date\", \"hardship_end_date\", \"payment_plan_start_date\", \"hardship_length\", \"hardship_dpd\", \"hardship_loan_status\", \"orig_projected_additional_accrued_interest\", \"hardship_payoff_balance_amount\", \"hardship_last_payment_amount\", \"disbursement_method\", \"debt_settlement_flag\", \"debt_settlement_flag_date\", \"settlement_status\", \"settlement_date\", \"settlement_amount\", \"settlement_percentage\", \"settlement_term\"]\n",
    "data = data_row.drop(columns=dropped_columns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Group the data by the \"loan_status\" column, and then counts the number of occurrences of each unique value in the \"loan_status\" column"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.groupby(\"loan_status\")[\"loan_status\"].count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For practical purposes, we decide to only consider loans with statuses containing \"Fully Paid\" or \"Charged Off\" as these indicate the loan is no longer active. The \"credit policy\" columns were also merged with their matching status."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data[\"purpose\"] = data[\"purpose\"].replace(\"educational\", \"other\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "credit_policy_related = \"Does not meet the credit policy. Status:\"\n",
    "len_credit_policy_related = len(credit_policy_related)\n",
    "\n",
    "def remove_credit_policy_related(status):\n",
    "    if credit_policy_related in str(status):\n",
    "        return status[len_credit_policy_related:]\n",
    "    else:\n",
    "        return status\n",
    "\n",
    "data[\"loan_status\"] = data[\"loan_status\"].apply(remove_credit_policy_related)\n",
    "\n",
    "rows_to_drop = data[(data[\"loan_status\"] != \"Charged Off\") & (data[\"loan_status\"] != \"Fully Paid\")].index\n",
    "data.drop(index=rows_to_drop, inplace=True)\n",
    "\n",
    "data.groupby(\"loan_status\")[\"loan_status\"].count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x='loan_status', data = data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The plot shows a histogram of the loan amount distribution in the dataset. The x-axis represents the loan amount and the y-axis represents the count (or frequency) of loans in the corresponding loan amount bin. It appears that the majority of loans fall within the range of 0 to 40,000, with a peak around 10,000."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize = (12,8))\n",
    "sns.histplot(x='loan_amnt', data = data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "#\n",
    "# correlation_matrix = data.corr(numeric_only=True)\n",
    "# # Display the correlation matrix\n",
    "# print(correlation_matrix)\n",
    "\n",
    "# A correlation matrix is a table showing the correlation coefficients between several variables. The values in the matrix range from -1 to 1 and show how strongly pairs of variables are related. A positive correlation means that as one variable increases, so does the other, while a negative correlation means that as one variable increases, the other decreases. A value of 0 indicates no correlation between the variables."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "#\n",
    "# # Define the threshold for high correlation\n",
    "# threshold = 0.9\n",
    "#\n",
    "# # Find highly correlated pairs of columns\n",
    "# corr_matrix = data.drop(columns=output_columns).corr().abs()\n",
    "# upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "# high_corr = [column for column in upper_tri.columns if any(upper_tri[column] > threshold)]\n",
    "# print(high_corr)\n",
    "#\n",
    "# # Drop highly correlated columns\n",
    "# data.drop(high_corr, axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data[output_columns].info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.groupby(\"term\")[\"term\"].count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a list of columns to one-hot encode\n",
    "onehot_coloumns = [\"term\"]\n",
    "\n",
    "# Strip leading/trailing whitespace from the term column\n",
    "data[\"term\"] = data[\"term\"].map(lambda term_str: term_str.strip())\n",
    "\n",
    "# Extract the numerical value from the term column and store it in a new column called term_num\n",
    "extract_num = lambda term_str: float(term_str[:2])\n",
    "data[\"term_num\"] = data[\"term\"].map(extract_num)\n",
    "\n",
    "# Remove the original term column from the list of output columns and add the new term_num column\n",
    "output_columns.remove(\"term\")\n",
    "output_columns.append(\"term_num\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "received = data[[\"total_rec_prncp\", \"total_rec_int\", \"total_rec_late_fee\", \"recoveries\"]].sum(axis=1) - data[\"collection_recovery_fee\"]\n",
    "expected = data[\"installment\"] * data[\"term_num\"]\n",
    "data[\"recovered_percentage\"] = received / expected\n",
    "\n",
    "data.groupby(\"loan_status\")[\"recovered_percentage\"].describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# set a maximum value of 1 for the recovered_percentage variable\n",
    "data[\"recovered_percentage\"] = np.where(\n",
    "  (data[\"loan_status\"] == \"Fully Paid\") | (data[\"recovered_percentage\"] > 1.0),\n",
    "  1.0,\n",
    "  data[\"recovered_percentage\"],\n",
    ")\n",
    "\n",
    "# group the data by loan_status and describe the recovered_percentage variable\n",
    "data.groupby(\"loan_status\")[\"recovered_percentage\"].describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "sns.kdeplot(\n",
    "  data=data[\"recovered_percentage\"][data[\"loan_status\"] == \"Charged Off\"],\n",
    "  label=\"Charged Off\",\n",
    "  fill=True,\n",
    ")\n",
    "plt.axis(xmin=0, xmax=1)\n",
    "plt.title('Recovered Percentage Distribution')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The verbose parameter is set to True to show the counts of non-null values.\n",
    "data.drop(columns=output_columns, inplace=True)\n",
    "data.info(verbose=True, show_counts=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The columns with a high number of null values can be categorized into three groups:\n",
    "1. derogatory/delinquency metrics, where null values indicate no such marks;\n",
    "2. metrics applicable only to joint applications, where null values indicate a single application;\n",
    "3. a group of 14 credit history-related columns with only 537,000 entries, which raises questions about their novelty.\n",
    "\n",
    "Additionally, I will include \"mths_since_recent_inq\" in the first category since its non-null count falls below the threshold of complete data at around 1,277,783, and null values here may indicate no recent inquiries."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "negative_mark_columns = [\"mths_since_last_delinq\", \"mths_since_last_record\", \"mths_since_last_major_derog\", \"mths_since_recent_bc_dlq\", \"mths_since_recent_inq\", \"mths_since_recent_revol_delinq\", \"mths_since_recent_revol_delinq\", \"sec_app_mths_since_last_major_derog\"]\n",
    "joint_columns = [\"annual_inc_joint\", \"dti_joint\", \"verification_status_joint\", \"revol_bal_joint\", \"sec_app_fico_range_low\", \"sec_app_fico_range_high\", \"sec_app_earliest_cr_line\", \"sec_app_inq_last_6mths\", \"sec_app_mort_acc\", \"sec_app_open_acc\", \"sec_app_revol_util\", \"sec_app_open_act_il\", \"sec_app_num_rev_accts\", \"sec_app_chargeoff_within_12_mths\", \"sec_app_collections_12_mths_ex_med\", \"sec_app_mths_since_last_major_derog\"]\n",
    "confusing_columns = [\"open_acc_6m\", \"open_act_il\", \"open_il_12m\", \"open_il_24m\", \"mths_since_rcnt_il\", \"total_bal_il\", \"il_util\", \"open_rv_12m\", \"open_rv_24m\", \"max_bal_bc\", \"all_util\", \"inq_fi\", \"total_cu_tl\", \"inq_last_12m\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert \"issue_d\" column to datetime format\n",
    "data[\"issue_d\"] = data[\"issue_d\"].astype(\"datetime64[ns]\")\n",
    "\n",
    "# Check the date range of confusing columns by selecting the confusing columns and the \"issue_d\" column,\n",
    "# removing rows with null values, and then computing the count, min and max values for \"issue_d\"\n",
    "data[confusing_columns + [\"issue_d\"]].dropna(axis=\"index\")[\"issue_d\"].agg(\n",
    "  [\"count\", \"min\", \"max\"]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Assuming 'data' is a DataFrame containing your data\n",
    "# # Make sure 'issue_d' is already converted to datetime format\n",
    "#\n",
    "# # Filter rows to only include data from 2018\n",
    "# data = data[(data['issue_d'] >= np.datetime64(\"2015-01-01\")) & (data['issue_d'] <= np.datetime64(\"2018-12-31\"))]\n",
    "#\n",
    "# # Reset the index of the new DataFrame\n",
    "# data.reset_index(drop=True, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Filter the \"issue_d\" column to only include dates from Dec 2015 onward and get the count, minimum, and maximum date values\n",
    "data[\"issue_d\"][data[\"issue_d\"] >= np.datetime64(\"2015-12-01\")].agg(\n",
    "  [\"count\", \"min\", \"max\"]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After examining the data, it appears that the 14 confusing columns are indeed newer metrics, and their usage only began in December 2015. However, even after that point, their usage is spotty. Despite this, I'm curious to see if these additional metrics would improve the accuracy of a machine learning model. So, after completing the data cleaning process, I will create a new dataset that contains only the rows with these new metrics and use it to build a new model that includes these metrics."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_metric_columns = confusing_columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To process the derogatory/delinquency metrics, I will follow Michael Wurm's suggestion and obtain the inverse of all the “months since recent/last” fields. This conversion will transform each field into a proxy for the frequency of the event, while allowing me to assign a 0 value to all the null fields (when an event has never occurred). For the “months since oldest” fields, I will set the null values to 0 and leave the remaining fields unchanged."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Identify columns with \"months since\" and \"months since recent\" in their name\n",
    "months_since_last_columns = [\n",
    "    col_name for col_name in data.columns if \"mths_since\" in col_name or \"mo_sin_rcnt\" in col_name\n",
    "]\n",
    "\n",
    "# For \"months since\" columns, if the value is 0, set it to 1 to avoid dividing by zero. If the value is already a number of months, take the inverse.\n",
    "for col_name in months_since_last_columns:\n",
    "    data[col_name] = [\n",
    "        0.0 if pd.isna(months) else 1 / 1 if months == 0 else 1 / months\n",
    "        for months in data[col_name]\n",
    "    ]\n",
    "\n",
    "# Rename inverse columns\n",
    "rename_mapper = {}\n",
    "for col_name in months_since_last_columns:\n",
    "    rename_mapper[col_name] = col_name.replace(\"mths_since\", \"inv_mths_since\").replace(\n",
    "        \"mo_sin_rcnt\", \"inv_mo_sin_rcnt\"\n",
    "    )\n",
    "data.rename(columns=rename_mapper, inplace=True)\n",
    "\n",
    "# Identify columns with \"months since oldest\" in their name\n",
    "months_since_oldest_columns = [\n",
    "    col_name for col_name in data.columns if \"mo_sin_old\" in col_name\n",
    "]\n",
    "\n",
    "# Replace null values in \"months since oldest\" columns with 0\n",
    "data.loc[:, months_since_oldest_columns].fillna(0, inplace=True)\n",
    "\n",
    "def replace_list_value(l, old_value, new_value):\n",
    "  i = l.index(old_value)\n",
    "  l.pop(i)\n",
    "  l.insert(i, new_value)\n",
    "\n",
    "# Replace columns in new_metric_columns with their inverse values\n",
    "replace_list_value(new_metric_columns, \"mths_since_rcnt_il\", \"inv_mths_since_rcnt_il\")\n",
    "replace_list_value(\n",
    "    joint_columns,\n",
    "    \"sec_app_mths_since_last_major_derog\",\n",
    "    \"sec_app_inv_mths_since_last_major_derog\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Take a look at joint loans, it seems there are newer metrics for joint applications as well."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "joint_loans = data[:][data[\"application_type\"] == \"Joint App\"]\n",
    "joint_loans[joint_columns].info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# List of columns with new metrics that only apply to joint applications\n",
    "joint_new_metric_columns = [\"revol_bal_joint\", \"sec_app_fico_range_low\", \"sec_app_fico_range_high\", \"sec_app_earliest_cr_line\", \"sec_app_inq_last_6mths\", \"sec_app_mort_acc\", \"sec_app_open_acc\", \"sec_app_revol_util\", \"sec_app_open_act_il\", \"sec_app_num_rev_accts\", \"sec_app_chargeoff_within_12_mths\", \"sec_app_collections_12_mths_ex_med\", \"sec_app_inv_mths_since_last_major_derog\"]\n",
    "\n",
    "# Check date range of joint loan data with new metrics\n",
    "joint_loans[joint_new_metric_columns + [\"issue_d\"]].dropna(axis=\"index\")[\"issue_d\"].agg(\n",
    "[\"count\", \"min\", \"max\"]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "joint_loans[\"issue_d\"].agg([\"count\", \"min\", \"max\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Adding \"application_type\" to the list of one-hot encoded columns.\n",
    "onehot_coloumns.append(\"application_type\")\n",
    "\n",
    "# Looping through a list of column names that appear in both the joint application and individual application dataframes and replacing the joint application values with the individual application values for applications that are not joint.\n",
    "for joint_col, indiv_col in zip(\n",
    "        [\"annual_inc_joint\", \"dti_joint\", \"verification_status_joint\"],\n",
    "        [\"annual_inc\", \"dti\", \"verification_status\"],\n",
    "):\n",
    "  data[joint_col] = [\n",
    "    joint_val if app_type == \"Joint App\" else indiv_val\n",
    "    for app_type, joint_val, indiv_val in zip(\n",
    "      data[\"application_type\"], data[joint_col], data[indiv_col]\n",
    "    )\n",
    "  ]\n",
    "\n",
    "data.info(verbose=True, show_counts=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cols_to_search = [\n",
    "  col for col in data.columns if col not in new_metric_columns + joint_new_metric_columns\n",
    "]\n",
    "data.dropna(axis=\"index\", subset=cols_to_search).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.dropna(axis=\"index\", subset=cols_to_search, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data[[\"earliest_cr_line\", \"sec_app_earliest_cr_line\"]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_credit_history_age(col_name):\n",
    "  earliest_cr_line_date = data[col_name].astype(\"datetime64[ns]\")\n",
    "  cr_hist_age_delta = data[\"issue_d\"] - earliest_cr_line_date\n",
    "  MINUTES_PER_MONTH = int(365.25 / 12 * 24 * 60)\n",
    "  cr_hist_age_months = cr_hist_age_delta / np.timedelta64(MINUTES_PER_MONTH, \"m\")\n",
    "  return cr_hist_age_months.map(\n",
    "    lambda value: np.nan if pd.isna(value) else round(value)\n",
    "  )\n",
    "\n",
    "\n",
    "cr_hist_age_months = get_credit_history_age(\"earliest_cr_line\")\n",
    "cr_hist_age_months"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data[\"earliest_cr_line\"] = cr_hist_age_months\n",
    "data[\"sec_app_earliest_cr_line\"] = get_credit_history_age(\n",
    "  \"sec_app_earliest_cr_line\"\n",
    ").astype(\"Int64\")\n",
    "data.rename(\n",
    "  columns={\n",
    "    \"earliest_cr_line\": \"cr_hist_age_mths\",\n",
    "    \"sec_app_earliest_cr_line\": \"sec_app_cr_hist_age_mths\",\n",
    "  },\n",
    "  inplace=True,\n",
    ")\n",
    "replace_list_value(\n",
    "  joint_new_metric_columns, \"sec_app_earliest_cr_line\", \"sec_app_cr_hist_age_mths\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "categorical_cols = [\"term\", \"emp_length\", \"home_ownership\", \"verification_status\", \"purpose\", \"verification_status_joint\"]\n",
    "for i, col_name in enumerate(categorical_cols):\n",
    "  print(\n",
    "    data.groupby(col_name)[col_name].count(),\n",
    "    \"\\n\" if i < len(categorical_cols) - 1 else \"\",\n",
    "  )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.drop(\n",
    "  columns=[\n",
    "    \"verification_status\",\n",
    "    \"verification_status_joint\",\n",
    "    \"issue_d\",\n",
    "    \"loan_status\",\n",
    "  ],\n",
    "  inplace=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "onehot_coloumns += [\"home_ownership\", \"purpose\"]\n",
    "ordinal_cols = {\n",
    "  \"emp_length\": [\n",
    "    \"< 1 year\",\n",
    "    \"1 year\",\n",
    "    \"2 years\",\n",
    "    \"3 years\",\n",
    "    \"4 years\",\n",
    "    \"5 years\",\n",
    "    \"6 years\",\n",
    "    \"7 years\",\n",
    "    \"8 years\",\n",
    "    \"9 years\",\n",
    "    \"10+ years\",\n",
    "  ]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_final = data.drop(columns=new_metric_columns + joint_new_metric_columns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query_df = data[[\"il_util\", \"total_bal_il\", \"total_il_high_credit_limit\"]].dropna(\n",
    "  axis=\"index\", subset=[\"il_util\"]\n",
    ")\n",
    "query_df[\"il_util_compute\"] = (\n",
    "        query_df[\"total_bal_il\"] / query_df[\"total_il_high_credit_limit\"]\n",
    ").map(lambda x: float(round(x * 100)))\n",
    "query_df[[\"il_util\", \"il_util_compute\"]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(query_df[\"il_util\"] == query_df[\"il_util_compute\"]).describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query_df[\"compute_diff\"] = abs(query_df[\"il_util\"] - query_df[\"il_util_compute\"])\n",
    "query_df[\"compute_diff\"][query_df[\"compute_diff\"] != 0].describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data[\"il_util_imputed\"] = [\n",
    "  True if pd.isna(util) & pd.notna(bal) & pd.notna(limit) else False\n",
    "  for util, bal, limit in zip(\n",
    "    data[\"il_util\"], data[\"total_bal_il\"], data[\"total_il_high_credit_limit\"]\n",
    "  )\n",
    "]\n",
    "new_metric_onehot_coloumns = [\"il_util_imputed\"]\n",
    "data[\"il_util\"] = [\n",
    "  0.0\n",
    "  if pd.isna(util) & pd.notna(bal) & (limit == 0)\n",
    "  else float(round(bal / limit * 100))\n",
    "  if pd.isna(util) & pd.notna(bal) & pd.notna(limit)\n",
    "  else util\n",
    "  for util, bal, limit in zip(\n",
    "    data[\"il_util\"], data[\"total_bal_il\"], data[\"total_il_high_credit_limit\"]\n",
    "  )\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from tensorflow.keras import Sequential, Input\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "\n",
    "def run_pipeline(\n",
    "        data, onehot_coloumns, ordinal_cols, batch_size, validate=True,\n",
    "):\n",
    "  X = data.drop(columns=[\"recovered_percentage\"])\n",
    "  y = data[\"recovered_percentage\"]\n",
    "  X_train, X_valid, y_train, y_valid = (\n",
    "    train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    if validate\n",
    "    else (X, None, y, None)\n",
    "  )\n",
    "\n",
    "  transformer = DataFrameMapper(\n",
    "    [\n",
    "      (onehot_coloumns, OneHotEncoder(drop=\"if_binary\")),\n",
    "      (\n",
    "        list(ordinal_cols.keys()),\n",
    "        OrdinalEncoder(categories=list(ordinal_cols.values())),\n",
    "      ),\n",
    "    ],\n",
    "    default=StandardScaler(),\n",
    "  )\n",
    "\n",
    "  X_train = transformer.fit_transform(X_train)\n",
    "  X_valid = transformer.transform(X_valid) if validate else None\n",
    "\n",
    "  input_nodes = X_train.shape[1]\n",
    "  output_nodes = 1\n",
    "\n",
    "  model = Sequential()\n",
    "  model.add(Input((input_nodes,)))\n",
    "  model.add(Dense(64, activation=\"relu\"))\n",
    "  model.add(Dropout(0.3, seed=0))\n",
    "  model.add(Dense(32, activation=\"relu\"))\n",
    "  model.add(Dropout(0.3, seed=1))\n",
    "  model.add(Dense(16, activation=\"relu\"))\n",
    "  model.add(Dropout(0.3, seed=2))\n",
    "  model.add(Dense(output_nodes))\n",
    "  model.compile(optimizer=\"adam\", loss=\"mean_squared_logarithmic_error\")\n",
    "\n",
    "  history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=10,\n",
    "    validation_data=(X_valid, y_valid) if validate else None,\n",
    "    verbose=1,\n",
    "  )\n",
    "\n",
    "  return history.history, model, transformer\n",
    "\n",
    "\n",
    "print(\"Model Result:\")\n",
    "history, final_model, final_transformer = run_pipeline(\n",
    "    data_final, onehot_coloumns, ordinal_cols, batch_size=128,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.lineplot(x=range(1, 11), y=history[\"loss\"], label=\"loss\")\n",
    "sns.lineplot(x=range(1, 11), y=history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.title(\"Model 1 loss metrics during training\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "final_model.save(\"loan_risk_model\")\n",
    "joblib.dump(final_transformer, \"data_transformer.joblib\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Exports for \"Can I Grade Loans Better than LendingClub?\"\n",
    "expected.rename(\"expected_return\", inplace=True)\n",
    "data_for_eval = data_final.join([data_row[[\"issue_d\", \"grade\", \"sub_grade\"]], expected])\n",
    "joblib.dump(data_for_eval, \"data_for_eval.joblib\")\n",
    "\n",
    "# Exports for \"Improving Loan Risk Prediction With Natural Language Processing\"\n",
    "data_for_nlp = data_final.join(data_row[[\"issue_d\", \"title\", \"desc\"]])\n",
    "joblib.dump(data_for_nlp, \"data_for_nlp.joblib\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
